1. Configure scala
Download scala-2.10.3.tgz from "http://www.scala-lang.org/download/" and set "SCALA_HOME" in ~/.bashrc as follows:
export SCALA_HOME=$HOME/workspace/scala-2.10.3
...
export PATH=$JAVA_HOME/bin:$SCALA_HOME/bin:$HADOOP_HOME/bin:$STORM_HOME/bin:$THRIFT_HOME/bin:$PATH


2. Build spark 
When building spark-0.9.0-incubating using command "sbt/sbt assembly" in top-level directory, I get the following:
[minglin@namenode spark-0.9.0-incubating]$ sbt/sbt assembly
Launching sbt from sbt/sbt-launch-0.12.4.jar
Error occurred during initialization of VM
Could not reserve enough space for object heap
Could not create the Java virtual machine.

From the error message, we know it is probably the memory setting for the jvm to lauch spark is too large.
I check script "sbt/sbt", and find the last line of the script as follows:
java \
  -Xmx1200m -XX:MaxPermSize=350m -XX:ReservedCodeCacheSize=256m \
  -jar ${JAR} \
  "$@"

I change "-Xmx1200m" to "-Xmx600m", then build works.
You can get the version of JDK by printing System.getProperty("sun.arch.data.model") or typing shell command "java -d32 -version".
I know the JDK installed is 32-bit.

Note: By default, Spark links to Hadoop 1.0.4. 
You can change this by setting the SPARK_HADOOP_VERSION variable when compiling:
SPARK_HADOOP_VERSION=2.2.0 sbt/sbt assembly
In addition, if you wish to run Spark on YARN, set SPARK_YARN to true:
SPARK_HADOOP_VERSION=2.0.5-alpha SPARK_YARN=true sbt/sbt assembly

Or to edit project/SparkBuild.scala and set HADOOP_VERSION.

4. Configure Spark cluster and start it
--- configure slaves and environment variables
conf/slaves:
datanode

cp conf/spark-env.sh.template conf/spark-env.sh
mkdir work_dir
spark_env.sh:
SPARK_MASTER_IP=namenode
SPARK_WORKER_DIR=$SPARK_HOME/work_dir

-- start the cluster
sbin/start-all.sh
web ui: http://namenode:8080/

5. test
--- run examples packaged with spark
bin/run-example org.apache.spark.examples.GroupByTest spark://`hostname`:7077

--- run an example built with sbt/sbt
mkdir -p example-scala-build/src/main/scala/spark/examples/
cp -af sbt example-scala-build/
cp examples/src/main/scala/spark/examples/GroupByTest.scala example-scala-build/src/main/scala/spark/examples/

mkdir project
cp project/build.properties project
vi project/plugins.sbt:
resolvers += Resolver.url("artifactory", url("http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases"))(Resolver.ivyStylePatterns)

resolvers += "Typesafe Repository" at "http://repo.typesafe.com/typesafe/releases/"

resolvers += "Spray Repository" at "http://repo.spray.cc/"

addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.9.2")

run.sh:
#!/bin/sh
SPARK_HOME=$HOME/workspace/spark-0.9.0-incubating

java -cp $SPARK_HOME/assembly/target/scala-2.10/spark-assembly-0.9.0-incubating-hadoop1.0.4.jar:$SPARK_HOME/example-scala-build/target/scala-2.10/groupbytest_2.10-0.1-SNAPSHOT.jar org.apache.spark.examples.GroupByTest spark://`hostname`:7077

Note: we can't use scala instead of java here, detailed information at: http://mail-archives.apache.org/mod_mbox/spark-dev/201312.mbox/<CAPh_B=ass2NcrN41t7KTSoF1SFGce=N57YMVyukX4hPcO5YN2Q@mail.gmail.com>
